# Navigating Adversarial Prompts to Secure Large Language Models

This repository contains research materials, code, and results for our study on the vulnerabilities of Large Language Models (LLMs) like ChatGPT and DALL-E 3 to adversarial prompts.

## Abstract

This study investigates the vulnerabilities of AI Large Language Models (LLMs) like ChatGPT and DALL-E 3 against sophisticated adversarial prompts. We highlight significant ethical and security risks and propose innovative testing methodologies to enhance AI defenses. Our findings emphasize the critical need for ongoing advancement in AI security protocols and ethical standards, aiming for a future where AI technologies are both secure and aligned with ethical guidelines.

## Research Overview

### Introduction

Our research explores the vulnerabilities of LLMs like ChatGPT and DALL-E 3 to adversarial prompts, identifying significant ethical and security risks. We've developed innovative testing methodologies that highlight the complex challenges AI models face.

### Background

The increasing use of LLMs like ChatGPT has raised significant security and ethical concerns. To uncover and mitigate AI vulnerabilities, our research team developed a robust testing framework that explores how prone these models are to adversarial attacks.

### Materials and Methods

Our research employs advanced adversarial prompts to probe the resilience of AI models. Through the strategic application of 'jailbreaking' techniques, we explore the ability of AI systems to withstand manipulative inputs.

### Key Findings

- Uncovered significant vulnerabilities in AI models like ChatGPT and DALL-E 3 when faced with sophisticated adversarial prompts.
- Identified key areas where these models could be exploited to produce unethical or harmful content.
- Developed the Response Quality Score (RQS) metric to quantify model resilience.

## Future Work

Moving forward, we will focus on multimodality, examining AI's integration of text, images, and voice to identify new vulnerabilities. This exploration aims to develop more secure, sophisticated AI systems resistant to complex adversarial attacks.

## Contributors

- Dejaun Gayle
- Brendan Hannon
- Dr. Y. Kumar (Faculty Advisor)
- Dr. J.J. Li (Faculty Advisor)

Department of Computer Science and Technology, Kean University, Union, NJ, USA

## Acknowledgments

This research was funded by NSF awards 1834620 and 2137791, Louis Stokes Alliances for Minority Participation (LSAMP) and Kean University internal funds.

## Contact

For more information, please contact Dejaun Gayle at gayledej@kean.edu.

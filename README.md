Navigating Adversarial Prompts to Secure Large Language Models
Abstract
This study investigates the vulnerabilities of AI Large Language Models (LLMs) like ChatGPT and DALL-E 3 against sophisticated adversarial prompts. We highlight significant ethical and security risks and propose innovative testing methodologies to enhance AI defenses. Our findings emphasize the critical need for ongoing advancement in AI security protocols and ethical standards, aiming for a future where AI technologies are both secure and aligned with ethical guidelines.
Introduction
Our research explores the vulnerabilities of LLMs like ChatGPT and DALL-E 3 to adversarial prompts, identifying significant ethical and security risks. We've developed innovative testing methodologies that highlight the complex challenges AI models face. This work underscores the urgent need for robust AI security measures and ethical standards to align AI technologies with secure and ethical practices for the future.
Background
The increasing use of LLMs like ChatGPT has raised significant security and ethical concerns. To uncover and mitigate AI vulnerabilities, our research team developed a robust testing framework that explores how prone these models are to adversarial attacks. This framework was initially presented at SERP 2023.
Materials and Methods
Our research employs advanced adversarial prompts to probe the resilience of AI models. Through the strategic application of 'jailbreaking' techniques, we explore the ability of AI systems to withstand manipulative inputs. This methodology enables a comprehensive assessment of AI vulnerabilities, laying the groundwork for the development of more robust AI defenses.
Results
Our investigation uncovered significant vulnerabilities in AI models like ChatGPT and DALL-E 3 when faced with sophisticated adversarial prompts. Through rigorous testing, we identified key areas where these models could be exploited to produce unethical or harmful content. We developed the Response Quality Score (RQS) metric to quantify model resilience, offering valuable insights into enhancing AI security measures.
Conclusion and Future Work
This research marks a pivotal step toward understanding and fortifying the defenses of LLMs against adversarial manipulations. We've highlighted the susceptibility of models like ChatGPT to sophisticated threats, emphasizing the crucial need for robust security frameworks and ethical standards in AI development.
Moving forward, we will focus on multimodality, examining AI's integration of text, images, and voice to identify new vulnerabilities. This exploration aims to develop more secure, sophisticated AI systems resistant to complex adversarial attacks. Our goal is to ensure AI's ethical use across diverse applications by enhancing its resilience.
Acknowledgements
This research was funded by NSF awards 1834620 and 2137791, Louis Stokes Alliances for Minority Participation (LSAMP) and Kean University internal funds.
Contributors

Dejaun Gayle
Brendan Hannon
Dr. Y. Kumar (Faculty Advisor)
Dr. J.J. Li (Faculty Advisor)

Department of Computer Science and Technology, Kean University, Union, NJ, USA
Repository Contents

README.md: This file, providing an overview of the research
poster/: Contains the research poster presented at Research Days 2024
